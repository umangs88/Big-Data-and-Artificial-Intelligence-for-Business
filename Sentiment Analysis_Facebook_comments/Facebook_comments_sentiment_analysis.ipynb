{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#loading the data\n",
    "df=pd.read_csv('facebook_comments.csv', header=None, names=['text','sentiment'] )\n",
    "#display the first five recors\n",
    "#df.head()\n",
    "df['sentiment']=df.sentiment.str.strip()\n",
    "#create lables\n",
    "df['labels'] = df.sentiment.map({'neutral':0,'negative':1,'positive':2})\n",
    "df.head()\n",
    "#convert text and lables into ndarrays using numpy \n",
    "training_text= df.text.values\n",
    "labels=df.labels.values\n",
    "print(type(training_text),type(labels))\n",
    "#training_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1999, 500) , (1999,)\n"
     ]
    }
   ],
   "source": [
    "# preprocess the loaded textual data, including removing stopwords, stemming, and tokenization\n",
    "# represent each document (i.e., comment) using TF-IDF strategy. The features are the top frequent unigrams across all comments\n",
    "# possible packages you might need are: scikit-learn, numpy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# tokenize and create a document-feature matrix X and a label vector Y\n",
    "vectorizer=TfidfVectorizer(stop_words='english', max_features=500, ngram_range=(1,1)) #unigram\n",
    "#using top frequent 500 unigram as features\n",
    "#apply this object the data to be transformed\n",
    "instances= vectorizer.fit_transform(training_text) #not the array its 1999x500 sparse matrix of type\n",
    "#convert to array\n",
    "X= instances.toarray()\n",
    "Y=labels\n",
    "# print out the shape of X and Y\n",
    "print(X.shape,',',Y.shape)\n",
    "#X\n",
    "#Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traditional Machine Learning Models: Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Umang\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - mean: 64.1832% (std: +/- 2.0458%)\n"
     ]
    }
   ],
   "source": [
    "# using 10-fold cross-validation to show the prediction accuracy\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# possible packages you might need are: scikit-learn, numpy\n",
    "k_fold= KFold(n_splits=10,shuffle=True, random_state=2020)\n",
    "rf_model= RandomForestClassifier(criterion='entropy',max_depth=2, random_state=2020) #empty model\n",
    "# there are 10 folds record accuracy for each\n",
    "rf_cvscores=[]\n",
    "for train_idx, test_idx in k_fold.split(X): # returns indexes of the records, once we have the index we can get the entire row vector\n",
    "    rf_model.fit(X[train_idx],Y[train_idx]) #train model on data, when you train you superwise provide X and Y\n",
    "    acc= rf_model.score(X[test_idx],Y[test_idx])                  # get the accuracy on the validatio set\n",
    "    rf_cvscores.append(acc)                                         #for every iteration appaned the list\n",
    "    \n",
    "print(\"Random Forest - mean: %.4f%% (std: +/- %.4f%%)\" % (np.mean(rf_cvscores)*100,np.std(rf_cvscores)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully connected feedforward Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda install -c pytorch pytorch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design your own network with the following requirements:\n",
    "# 1. Having dropout\n",
    "# 2. Separate the dataset into training and validation (80-20%)\n",
    "# 3. The prediction accuracy on the validation set should be at least 50% for this 3-\n",
    "# possible packages you might need are: scikit-learn, numpy, torch\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the train loader and validation loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert your numpy array to TensorDataset and create a data loader for training and\n",
    "# some hyperparameters: input dimension, output dimension, batch size, number of epoc\n",
    "batch_size = 16 #for each epoch 16 batches to load a batch we need to create a data loader convert arrays into tensors\n",
    "epochs = 50\n",
    "lr = 1e-4\n",
    "indim = X.shape[1] #input dimensionality \n",
    "outdim = 3  #output dimensionality 3, {0,1,2}, 3 class classification problem\n",
    "drate = 0.7 #droprate\n",
    "#X.shape[0]\n",
    "#step1: Create X.tensor & Y.tensor from numpy array \n",
    "X_tensor= torch.from_numpy(X)\n",
    "Y_tensor= torch.from_numpy(Y)\n",
    "#Step2: Creat a dataset, Vertiaclly concatinate them together(when loader is called this dataset will be included in each batch)\n",
    "dataset= TensorDataset(X_tensor,Y_tensor)\n",
    "#Step3: Seperate the dataset into train and test set\n",
    "train_size= int(0.8*len(dataset))\n",
    "train_size\n",
    "#test_size=int(0.2*len(dataset))\n",
    "#test_size\n",
    "test_size=len(dataset)-train_size\n",
    "#test_size\n",
    "train_dataset, test_dataset= torch.utils.data.random_split(dataset,[train_size,test_size]) #using random split create training and testing dataset\n",
    "#len(train_dataset)\n",
    "#Step4: Create train loader and test loader \n",
    "train_loader= DataLoader(train_dataset,batch_size= batch_size, shuffle=True)\n",
    "test_loader= DataLoader(test_dataset,batch_size= batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create your model/network\n",
    "#from class many objects can be instanciated,SentimentNetwork is a subclass inherited from as super/root class in pytorch from NN which is nn.Module\n",
    "class SentimentNetwork(nn.Module): \n",
    "    def __init__(self, input_dim, output_dim, drate):\n",
    "        super(SentimentNetwork,self).__init__()\n",
    "        #using the self. variable becomes a class variable as the property of the class if not its a local variable\n",
    "        #suppose hidden layer 1 has 500 neurons and layer 2 has 50\n",
    "        self.fc1= nn.Linear(input_dim, 100)\n",
    "        self.do1=nn.Dropout(drate)\n",
    "        self.fc2= nn.Linear(100, 50)\n",
    "        self.do2=nn.Dropout(drate)\n",
    "        self.opt= nn.Linear(50, output_dim)\n",
    "         \n",
    "        \n",
    "    #forward function is to get the predicated outout, calculate the loss\n",
    "    # x could be an instance, batch, many instances\n",
    "    #pass x through hidden layer, call the hidden layers defined above\n",
    "    def forward(self,x):\n",
    "        #x with information aggregation and transformation\n",
    "        x= F.relu(self.fc1(x))\n",
    "        x= self.do1(x)\n",
    "        x= F.relu(self.fc2(x))\n",
    "        x= self.do2(x)\n",
    "        x= F.log_softmax(self.opt(x))\n",
    "        x=torch.exp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentNetwork(\n",
      "  (fc1): Linear(in_features=500, out_features=100, bias=True)\n",
      "  (do1): Dropout(p=0.7, inplace=False)\n",
      "  (fc2): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (do2): Dropout(p=0.7, inplace=False)\n",
      "  (opt): Linear(in_features=50, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# create a model\n",
    "model = SentimentNetwork(indim,outdim,drate)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a training function to train the model and an evaluation function to evaluate the performance on the separate validation set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the loss function and optimizer\n",
    "#call crossentropyloss and adam    \n",
    "criterion= nn.CrossEntropyLoss() \n",
    "#prediction: batch_size X output_dim, truth: batch_size X 1\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=lr)\n",
    "                                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a training process function for one epoch each\n",
    "def train(model, train_loader, optimizer, criterion):\n",
    "    epoch_loss, epoch_acc = 0.0,0.0 # the loss and accuracy for each epoch\n",
    "    acc=0\n",
    "    model.train()\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        #zero gradient\n",
    "        optimizer.zero_grad()\n",
    "        #prediction= calculate the predicted output for the current batch batch_x\n",
    "        net_out= model(batch_x.float()) #models predicted output \n",
    "        #convert torch variable to numpy: predictions.detach().numpy()\n",
    "        #net_out.detach.numpy()\n",
    "        #loss= calculate the loss for the current batch using predictions and batch_y\n",
    "        loss= criterion(net_out, batch_y)\n",
    "        #to get the value from tensor to compare\n",
    "        loss_val= loss.item()\n",
    "        #acc= calculate the accuracy using predictions(dimensionality is batch_size x output_dim) and batch_y(dimensionality= batch_sizex1)\n",
    "        predicted= net_out.argmax(1)\n",
    "        acc=predicted.eq(batch_y).sum().item ()\n",
    "        # backpropogate   #iteration can be applied to tensor \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        epoch_loss += loss.item()  #to get the value from tensor\n",
    "        epoch_acc += acc\n",
    "       \n",
    "        #calculate the average epoch_loss and epoch_acc \n",
    "    average_epoch_loss = epoch_loss/len(train_dataset)\n",
    "    average_epoch_acc = epoch_acc/len(train_dataset)\n",
    "    return   average_epoch_loss, average_epoch_acc\n",
    "\n",
    "\n",
    "# define a validation/evaluation process function\n",
    "def evaluate(model, test_loader, criterion):\n",
    "    epoch_loss, epoch_acc = 0.0,0.0 # the loss and accuracy for each epoch\n",
    "    acc=0\n",
    "    model.eval()\n",
    "    #no need for any parameter updating, done for this epoch only evaluate on the parameters learned upto this point no gradient\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            #predict the output\n",
    "            net_out= model(batch_x.float())\n",
    "            #calculate the loss\n",
    "            loss= criterion(net_out, batch_y)\n",
    "            loss_val= loss.item()\n",
    "            #calculate the accuracy\n",
    "            predicted= net_out.argmax(1)\n",
    "            acc=predicted.eq(batch_y).sum().item ()\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc\n",
    "            #calculate the average epoch_loss and epoch_acc\n",
    "    average_epoch_loss = epoch_loss/len(test_dataset)\n",
    "    average_epoch_acc = epoch_acc/len(test_dataset)\n",
    "    return average_epoch_loss, average_epoch_acc\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main starting point: train the model and evaluate the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Umang\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss: 0.0686 | Train Acc: 0.3846\n",
      "\t Val. Loss: 0.0684 |  Val. Acc: 0.6650\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.0682 | Train Acc: 0.5422\n",
      "\t Val. Loss: 0.0678 |  Val. Acc: 0.6650\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.0676 | Train Acc: 0.6179\n",
      "\t Val. Loss: 0.0670 |  Val. Acc: 0.6650\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.0667 | Train Acc: 0.6341\n",
      "\t Val. Loss: 0.0657 |  Val. Acc: 0.6650\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.0650 | Train Acc: 0.6348\n",
      "\t Val. Loss: 0.0633 |  Val. Acc: 0.6650\n",
      "Epoch: 06\n",
      "\tTrain Loss: 0.0627 | Train Acc: 0.6354\n",
      "\t Val. Loss: 0.0603 |  Val. Acc: 0.6650\n",
      "Epoch: 07\n",
      "\tTrain Loss: 0.0604 | Train Acc: 0.6354\n",
      "\t Val. Loss: 0.0577 |  Val. Acc: 0.6650\n",
      "Epoch: 08\n",
      "\tTrain Loss: 0.0590 | Train Acc: 0.6354\n",
      "\t Val. Loss: 0.0562 |  Val. Acc: 0.6650\n",
      "Epoch: 09\n",
      "\tTrain Loss: 0.0576 | Train Acc: 0.6354\n",
      "\t Val. Loss: 0.0555 |  Val. Acc: 0.6650\n",
      "Epoch: 10\n",
      "\tTrain Loss: 0.0572 | Train Acc: 0.6354\n",
      "\t Val. Loss: 0.0550 |  Val. Acc: 0.6650\n",
      "Epoch: 11\n",
      "\tTrain Loss: 0.0571 | Train Acc: 0.6354\n",
      "\t Val. Loss: 0.0548 |  Val. Acc: 0.6650\n",
      "Epoch: 12\n",
      "\tTrain Loss: 0.0566 | Train Acc: 0.6354\n",
      "\t Val. Loss: 0.0546 |  Val. Acc: 0.6650\n",
      "Epoch: 13\n",
      "\tTrain Loss: 0.0566 | Train Acc: 0.6354\n",
      "\t Val. Loss: 0.0545 |  Val. Acc: 0.6650\n",
      "Epoch: 14\n",
      "\tTrain Loss: 0.0561 | Train Acc: 0.6354\n",
      "\t Val. Loss: 0.0543 |  Val. Acc: 0.6650\n",
      "Epoch: 15\n",
      "\tTrain Loss: 0.0558 | Train Acc: 0.6354\n",
      "\t Val. Loss: 0.0542 |  Val. Acc: 0.6650\n",
      "Epoch: 16\n",
      "\tTrain Loss: 0.0557 | Train Acc: 0.6354\n",
      "\t Val. Loss: 0.0540 |  Val. Acc: 0.6650\n",
      "Epoch: 17\n",
      "\tTrain Loss: 0.0555 | Train Acc: 0.6354\n",
      "\t Val. Loss: 0.0538 |  Val. Acc: 0.6650\n",
      "Epoch: 18\n",
      "\tTrain Loss: 0.0553 | Train Acc: 0.6354\n",
      "\t Val. Loss: 0.0536 |  Val. Acc: 0.6650\n",
      "Epoch: 19\n",
      "\tTrain Loss: 0.0548 | Train Acc: 0.6360\n",
      "\t Val. Loss: 0.0534 |  Val. Acc: 0.6650\n",
      "Epoch: 20\n",
      "\tTrain Loss: 0.0546 | Train Acc: 0.6360\n",
      "\t Val. Loss: 0.0532 |  Val. Acc: 0.6650\n",
      "Epoch: 21\n",
      "\tTrain Loss: 0.0541 | Train Acc: 0.6366\n",
      "\t Val. Loss: 0.0529 |  Val. Acc: 0.6650\n",
      "Epoch: 22\n",
      "\tTrain Loss: 0.0537 | Train Acc: 0.6379\n",
      "\t Val. Loss: 0.0526 |  Val. Acc: 0.6650\n",
      "Epoch: 23\n",
      "\tTrain Loss: 0.0533 | Train Acc: 0.6442\n",
      "\t Val. Loss: 0.0523 |  Val. Acc: 0.6675\n",
      "Epoch: 24\n",
      "\tTrain Loss: 0.0532 | Train Acc: 0.6517\n",
      "\t Val. Loss: 0.0520 |  Val. Acc: 0.6675\n",
      "Epoch: 25\n",
      "\tTrain Loss: 0.0525 | Train Acc: 0.6542\n",
      "\t Val. Loss: 0.0517 |  Val. Acc: 0.6825\n",
      "Epoch: 26\n",
      "\tTrain Loss: 0.0522 | Train Acc: 0.6817\n",
      "\t Val. Loss: 0.0513 |  Val. Acc: 0.6900\n",
      "Epoch: 27\n",
      "\tTrain Loss: 0.0518 | Train Acc: 0.6954\n",
      "\t Val. Loss: 0.0510 |  Val. Acc: 0.6950\n",
      "Epoch: 28\n",
      "\tTrain Loss: 0.0513 | Train Acc: 0.7167\n",
      "\t Val. Loss: 0.0507 |  Val. Acc: 0.7100\n",
      "Epoch: 29\n",
      "\tTrain Loss: 0.0510 | Train Acc: 0.7355\n",
      "\t Val. Loss: 0.0504 |  Val. Acc: 0.7225\n",
      "Epoch: 30\n",
      "\tTrain Loss: 0.0504 | Train Acc: 0.7530\n",
      "\t Val. Loss: 0.0501 |  Val. Acc: 0.7450\n",
      "Epoch: 31\n",
      "\tTrain Loss: 0.0500 | Train Acc: 0.7736\n",
      "\t Val. Loss: 0.0498 |  Val. Acc: 0.7600\n",
      "Epoch: 32\n",
      "\tTrain Loss: 0.0500 | Train Acc: 0.7736\n",
      "\t Val. Loss: 0.0495 |  Val. Acc: 0.7800\n",
      "Epoch: 33\n",
      "\tTrain Loss: 0.0497 | Train Acc: 0.7811\n",
      "\t Val. Loss: 0.0493 |  Val. Acc: 0.7825\n",
      "Epoch: 34\n",
      "\tTrain Loss: 0.0490 | Train Acc: 0.7986\n",
      "\t Val. Loss: 0.0490 |  Val. Acc: 0.7850\n",
      "Epoch: 35\n",
      "\tTrain Loss: 0.0488 | Train Acc: 0.7999\n",
      "\t Val. Loss: 0.0487 |  Val. Acc: 0.7875\n",
      "Epoch: 36\n",
      "\tTrain Loss: 0.0486 | Train Acc: 0.8093\n",
      "\t Val. Loss: 0.0485 |  Val. Acc: 0.7925\n",
      "Epoch: 37\n",
      "\tTrain Loss: 0.0480 | Train Acc: 0.8230\n",
      "\t Val. Loss: 0.0482 |  Val. Acc: 0.8000\n",
      "Epoch: 38\n",
      "\tTrain Loss: 0.0480 | Train Acc: 0.8186\n",
      "\t Val. Loss: 0.0479 |  Val. Acc: 0.8050\n",
      "Epoch: 39\n",
      "\tTrain Loss: 0.0472 | Train Acc: 0.8305\n",
      "\t Val. Loss: 0.0476 |  Val. Acc: 0.8100\n",
      "Epoch: 40\n",
      "\tTrain Loss: 0.0473 | Train Acc: 0.8318\n",
      "\t Val. Loss: 0.0474 |  Val. Acc: 0.8150\n",
      "Epoch: 41\n",
      "\tTrain Loss: 0.0471 | Train Acc: 0.8361\n",
      "\t Val. Loss: 0.0472 |  Val. Acc: 0.8225\n",
      "Epoch: 42\n",
      "\tTrain Loss: 0.0466 | Train Acc: 0.8443\n",
      "\t Val. Loss: 0.0470 |  Val. Acc: 0.8275\n",
      "Epoch: 43\n",
      "\tTrain Loss: 0.0466 | Train Acc: 0.8412\n",
      "\t Val. Loss: 0.0469 |  Val. Acc: 0.8250\n",
      "Epoch: 44\n",
      "\tTrain Loss: 0.0459 | Train Acc: 0.8537\n",
      "\t Val. Loss: 0.0467 |  Val. Acc: 0.8275\n",
      "Epoch: 45\n",
      "\tTrain Loss: 0.0460 | Train Acc: 0.8474\n",
      "\t Val. Loss: 0.0465 |  Val. Acc: 0.8325\n",
      "Epoch: 46\n",
      "\tTrain Loss: 0.0457 | Train Acc: 0.8487\n",
      "\t Val. Loss: 0.0463 |  Val. Acc: 0.8300\n",
      "Epoch: 47\n",
      "\tTrain Loss: 0.0458 | Train Acc: 0.8549\n",
      "\t Val. Loss: 0.0462 |  Val. Acc: 0.8325\n",
      "Epoch: 48\n",
      "\tTrain Loss: 0.0453 | Train Acc: 0.8593\n",
      "\t Val. Loss: 0.0461 |  Val. Acc: 0.8325\n",
      "Epoch: 49\n",
      "\tTrain Loss: 0.0449 | Train Acc: 0.8612\n",
      "\t Val. Loss: 0.0459 |  Val. Acc: 0.8325\n",
      "Epoch: 50\n",
      "\tTrain Loss: 0.0450 | Train Acc: 0.8568\n",
      "\t Val. Loss: 0.0458 |  Val. Acc: 0.8325\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# real training and evaluation process\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, test_loader, criterion)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.4f} |  Val. Acc: {valid_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### as the number of epochs increased the accuracy of the model improved with 64% for 5 to ~80% for 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
